# Machine Learning Regression Algorithms (MLRA) {#mlra}

Machine Learning Regression Algorithms (MLRA) are non-linear and non-parametric techniques that can be used to retrieve forest biophysical and biochemical parameters. Apart from non-parametric methods, there are also parametric techniques that can be used for the same purpose. However, the main advantage of the non-parametric methods over the parametric ones is the fact that they often do not assume any underlying relationships between the response (e.g. plant traits) and predictor (e.g. canopy reflectance) variables. These models do not rely on any linearity, which makes them very compelling techniques that can be applied in various research fields [@sinha2020estimation; @rivera2015emulator; @verrelst2019quantifying].

According to the extensive review study by @verrelst2019quantifying, Decision Trees (DT), Kernel-Based Machine Learning Regression Methods (KBMLRM) and Artificial Neural Networks (ANN) are the three most widely used Machine Learning  (ML) families for retrieval of plant biophysical and biochemical variables in remote sensing. This chapter focuses on the most commonly used ML techniques and briefly reviews them.

## Decision Trees (DT) {#dt}

Decision Tree (DT) methods rely on trees and branches to illustrate the outcome of each decision [@verrelst2019quantifying]. The Random Forest (RF) is a non-parametric learning model and it is also considered to be an ensemble model [@breiman2001random]. Essentially, the Random Forest Regression (RFR) algorithm builds multiple small regression trees where each tree has a vote on the prediction. Based on the votes of each tree, a final prediction is made [@breiman2001random; @powell2010quantification]. The major advantage of the Random Forest model is that it is not sensitive to overfitting [@breiman2001random; @powell2010quantification; @verrelst2019quantifying], which is a well-known phenomenon challenging many ML algorithms' performance. Apart from that, the RF algorithm can successfully deal with large amounts of training data as well as outliers and noise in the training data. These properties are the reason why the RF model is an attractive method for plant trait mapping applications in remote sensing [@verrelst2019quantifying].

Essentially, there are two main parameters of the RF algorithm. These are the number of trees (ntree) and number of variables (mtry) that will be randomly sampled at every split [@wang2018estimation]. @breiman2001random suggests that a tree count of 500 (ntree = 500) may be sufficient in many cases.

The applicability of the RFR algorithm for retrieval of plant traits has been demonstrated in many studies. For example, @ali2020machine indicates that, when compared to the traditional Look Up Table (LUT) method with merit function, the RFR's retrieval accuracy was higher. In the studies by @han2016hyperspectral and @pullanagari2016mapping, RFR was compared to the other widely used ML algorithms (e.g. SVM). In these studies, no method was found to be significantly superior to one another, indicating that all the widely used ML methods are highly competitive [@verrelst2019quantifying].

## Kernel-Based Machine Learning Regression Methods (KBMLRM) {#kernel}

Kernel-based learning algorithms rely on turning the current dimension of the data into a higher dimension and solving non-linear problems using a kernel function. Kernel methods offer very flexible implementation and they can be efficiently utilized as long as the linear problem can be solved in terms of dot products (linear algebra) [@verrelst2019quantifying]. According to the review study by @gomez2011review, kernel-based methods play a very important role in the remote sensing field. Kernel-based models can efficiently deal with small number of training samples (training data), very high-dimensional and noisy data sets. Unlike kernel-based algorithms, other machine learning methods, such as neural networks (NN), are known to be sensitive to the noise and high dimensions of the data, and importantly, they tend to perform very poorly when large training data sets are not available. All of these properties make kernel-based algorithms very attractive alternatives for remote sensing researchers as the mentioned issues that kernel-based algorithms handle are well-known for remote sensing data [@gomez2011review].

Support Vector Regression (SVR) [@drucker1997support] is one of the most widely used KBMLRM [@verrelst2019quantifying]. One of the advantages of Support Vector Machines (SVM) (@boser1992training) over other ML methods such as ANN is that it mainly relies on minimizing the risk function, as opposed to trying to minimize the error in the training data set. Artificial neural networks, however, tries to minimize the error function, which makes them very susceptible to overfitting to the training data [@karimi2008application]. The applicability of SVM to estimate plant biophysical traits has been demonstrated in various studies. For example, @karimi2008application and @yang2011estimating used SVM to estimate plant biophysical parameters from hyperspectral remote sensing data. A study by @tuia2011multioutput demonstrated the successful use of multioutput SVR in order to retrieve the estimated biophysical parameters. Various studies compared the performance of SVR to the other ML methods. For example, @pullanagari2016mapping used different methods to map macro and micro nutrients from hyperspectral data. This study concluded that although SVR performed better for a certain parameter retrieval, this algorithm was not superior to the other ML methods in general.

Another widely used kernel-based ML method is Gaussian Process Regression (GPR). GPR is a Bayesian, non-parametric, and probabilistic ML method that provides important insights for retrieval of plant traits from remote sensing data [@camps2016survey; @camps2019perspective]. The major advantage of GPR over the other widely used ML algorithms is that GP models provide confidence intervals for the estimations [@berger2020retrieval]. In other words, besides providing very good results, GPR can also give information about how much uncertainty (e.g. error bars) exists in the predictions or estimations of the retrieved parameters. GPR can easily deal with different types of data and it can be implemented in a way that efficiently handles noise in the data. These issues commonly occur in remote sensing data, which makes the GPR algorithm very interesting for the remote sensing community [@camps2016survey; @camps2019perspective]. Successful applications of GPR to retrieve plant biophysical and biochemical variables have been demonstrated in the literature. A relatively recent study by @berger2020retrieval used two versions of GPR (homoscedastic and heteroscedastic) for retrieval of plant nitrogen content. This study showed that the both versions of GPR performed well for the retrieval of plant nitrogen content. @upreti2019comparison used different ML methods to retrieve plant traits from the multispectral remote sensing satellite data Seninel-2. They found GPR to be the best performing algorithm within the cross-validation stage, but not in general [@upreti2019comparison]. A study by @caicedo2014toward compared different ML models in terms of accuracy and concluded that GPR yielded the most accurate results. However, it is important to note that GPR does not come without drawbacks. For example, one of the main challenges of GPR is that it is computationally costly despite advances [@camps2019perspective].

## Artificial Neural Networks (ANN) {#ann}

Artificial Neural Networks (ANN) is a powerful ML method that consists of neurons and layers [@simon1999neural]. Usually, the ANN has three main components: an input layer, (a) hidden layer(s) and an output layer [@jensen1999predictive; @quan2017radiative]. A typical mechanism for training an ANN is that first a training data set is given to the model as an input layer and then, the model trains and predicts output. Once the predicted output is available, the current predicted output is compared to the true output so that the weight parameters can be adjusted in a way that the predicted and true output are closely similar [@ingram2005mapping; @quan2017radiative]. This learning process can be repeated multiple times until similarity between the predicted and true outputs is at a certain threshold (e.g. convergence). This threshold can be defined by the user depending on their preferences [@jensen1999predictive; @quan2017radiative]. 

Main advantages of ANN are their simplicity, computational efficiency and their ability to learn from data where linearity is not assumed [@schlerf2006inversion; @walczak2019artificial]. Furthermore, ANNs do not require any prior information about the distribution that the data come from unlike the traditional statistical methods [@walczak2019artificial].

ANNs have been utilized in the remote sensing domain, especially for plant property mapping, since the 1990s [@verrelst2019quantifying]. There are many studies in the literature that show the superiority of the ANN methods to more traditional statistical methods. For example, studies by @malenovsky2013retrieval and @kalacska2015estimation showed that compared to the traditional statistical and vegetation index (VI) methods, ANN yielded more accurate results for retrieval of plant parameters from hyperspectral remote sensing data. @neinavaz2016retrieval compared ANN's performance to other methods (e.g. linear parametric) and found that ANN was superior to the compared methods for retrieval of an important plant variable, LAI. Moreover, a recent study by @danner2021efficient used four widely used ML methods to retrieve plant traits and compared the results of each method. In general, ANN was found to be the best performing ML method among the others, considering the models' efficiency, robustness, accuracy, and computational time. 

Despite being a powerful ML method, ANN comes with drawbacks. There are many descriptors in a typical ANN that may be correlated to each other. Because of that, there is a risk of the model being stuck in the local minima. Another well-known challenge of ANN is that it is susceptible to overfitting. Fortunately, there have been many methods developed to deal with these problems (e.g. regularization) [@ghasemi2018neural]. A study by @schlerf2006inversion points out another potential drawback, which is the fact that the ANN model may behave unpredictably when the model does not represent the spectral properties of the target variable well.

## Challenges of Machine Learning Regression Algorithms (MLRA) {#chml}

There are common issues that almost all MLRAs are challenged with. Hyperspectral remote sensing data come with many bands that may be highly correlated and noisy. This problem is a well-known problem and it is also called the "Hughes phenomenon" [@hughes1968mean] or the "curse of dimensionality" [@danner2021efficient]. One of the main problems is that MLRAs' computational cost becomes higher with an increased amount of training data. Most MLRAs require high computational power for very costly training phases. Apart from that, many highly correlated bands may indicate highly redundant data which cause difficulties for statistical and ML methods [@rivera2017hyperspectral].

There are some methods that can be used to efficiently deal with the "curse of dimensionality" problem. Some of the most commonly used methods for dealing with this issue involve reducing the space (dimension) of the original data. @rivera2017hyperspectral indicate that this issue can be solved in two different domains. The first method involves choosing the samples that provide most of the information compared to the other samples. The second domain involves the use of feature selection or dimensionality reduction (DR) techniques. In the first technique, the amount of samples is minimized but accuracy is still a priority [@rivera2017hyperspectral].

The second set of techniques (DR) tries to reduce the original data space while keeping most of the information available. In other words, DR techniques convert a large number of features with a high amount of redundancy into a much smaller feature set with no or as little redundancy as possible [@lee2007nonlinear]. Two of the commonly used DR techniques to compress the spectral reflectance data in remote sensing are so-called wavelet transfor (WT) and principal component analysis (PCA) [@ke2016estimating].

After reducing the dimension of the original data, new and low-dimensional data can be used to train ML models. MLRAs can be combined with PCA to reduce the computational cost and avoid the "curse of dimensionality" problem. There have been studies comparing the performance of ML methods when combined with DR techniques to feature selection or VI based ML applications. For example, @liu2017hyperspectral compared the performance of PCA based ANN with VI based ANN and concluded that using PCA before training is superior to using VI in the training phase. However, it is also important to note that PCA is not the only DR technique, although it is the most commonly used DR technique in plant retrieval studies by remote sensing researchers. A study by @rivera2017hyperspectral reviews and compares different DR techniques and concludes that PCA is not always the best performing DR technique and the use of other techniques may yield better results in some cases.

In general, most ML algorithms need a large amount of training data to learn from the training data and generalize well to the data that were not used in the training phase (e.g. test data). This leads to another challenging property of ML for retrieval of plant biophysical traits. This is because of the fact that it is extremely difficult to collect enough training data for plant traits in the field for a ML method to perform well. It requires various *in-situ* field techniques and these techniques need to be used in a limited time-frame. It is still not guaranteed that training data collected in the field can generalize well to other fields [@danner2021efficient]. One way to overcome this problem is to use the so-called hybrid-machine learning method.









